{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Credit Card Fraud using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll be leveraging the power of deep learning to solve a key issue that credit card companies often have to address, namely detecting fradulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.model_selection as model_selection\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CreditCard = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total no. of records in the dataset are\", CreditCard.shape[0])\n",
    "print(\"Total features in the dataset are\", CreditCard.shape[1])\n",
    "CreditCard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## To check missing values in the dataset.\n",
    "CreditCard.isnull().values.any() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that our dataset has no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rename Class\n",
    "CreditCard.rename(columns ={'Class': \"isFraud\"}, inplace = True)\n",
    "CreditCard = CreditCard.applymap(lambda x: x.replace(\"'\", \"\") if (isinstance(x, str)) else x)\n",
    "CreditCard['isFraud'] = pd.to_numeric(CreditCard['isFraud'])\n",
    "\n",
    "# fraudulent Transactions Percentage\n",
    "fraud_per = CreditCard[CreditCard.isFraud == 1].isFraud.count() / CreditCard.isFraud.count()*100\n",
    "print(\"Percentage of Fraudulent Transactions in the dataset are {:.2f} %\".format(fraud_per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the dataset from Kaggle and it contains two days worth of transactions by European cardholders. Due to cdonfidential nature of the data, a PCA transformation was done on 28 features and we have no information on what those features are. The only features that haven't undergone this transformation and we can identify them are 'Time', 'Amount', and 'Class'.\n",
    "\n",
    "Time represents the seconds elapsed between each transaction and the first transaction in the dataset. 'Amount denotes the amount of transaction anjd 'Class' refers to out target variable with 0 referring to a normal transaction and 1 referring to a fraudulent one.\n",
    "\n",
    "It is important to note here that the target variable's instances are imbalanced. Only 0.17% of transactions are fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do fraudulent transactions occur more often during certain time?\n",
    "f, (ax1, ax2) = plt.subplots(2,1, sharex = True)\n",
    "f.suptitle('Time of transaction vs Amount by class')\n",
    "\n",
    "ax1.scatter(CreditCard[CreditCard.isFraud == 1].Time, CreditCard[CreditCard.isFraud == 1].Amount)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.scatter(CreditCard[CreditCard.isFraud == 0].Time, CreditCard[CreditCard.isFraud == 0].Amount)\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Time in Seconds')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem like the time of transaction really matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining x and y\n",
    "x = CreditCard.iloc[:,:-1].values\n",
    "y = CreditCard.iloc[:,:1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining training and tesing set\n",
    "## Train-Test split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(x, y, test_size =0.1, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardising the dataset as this would speedup the training process\n",
    "\n",
    "## Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, this dataset is highly imbalanced. We'll address this issue using Synthetic Minority Oversampling Technique (SMOTE). This technique creates artificial minority class samples by replicating them. In this case it will create synthetic fraud instances and so corrects the imbalance in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SMOTE\n",
    "# sm = SMOTE(random_state = 2)\n",
    "# X_train_SMOTE, y_train_SMOTE = sm.fit_resample(X_train,y_train)\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_train_SMOTE, y_train_SMOTE = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# ## SMOTE plot\n",
    "# pd.Series(y_train_SMOTE).value_counts().plot(kind = \"bar\")\n",
    "# plt.title(\"Balanced Dataset\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ANN Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve come to this number of neurons and layers in our network using a trial and error approach. We also used ReLU as our activation function for the hidden layers and a sigmoid function for our output layer. We've used multiple droput layers to prevent our network overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DNN\n",
    "model = keras.Sequential([\n",
    "    tf.keras.layers.Dense(input_dim = 30, units =128, activation =\"relu\"),\n",
    "    tf.keras.layers.Dense(units = 64, activation = \"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units = 32, activation =\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units = 32, activation =\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units = 16, activation =\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units = 1, activation =\"sigmoid\")])\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metrics\n",
    "metrics = [\n",
    "    tf.keras.metrics.Accuracy(name = \"Accuracy\"),\n",
    "    tf.keras.metrics.Precision(name = \"Precision\"),\n",
    "    tf.keras.metrics.Recall(name =\"Recall\")]\n",
    "\n",
    "## Compiling and Fiting the model\n",
    "model.compile(optimizer =\"adam\",loss = \"binary_crossentropy\",\n",
    "             metrics = metrics)\n",
    "model.fit(X_train_SMOTE, y_train_SMOTE, batch_size = 32, epochs = 50)\n",
    "\n",
    "print(\"Evaluate on test data\")\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"test loss, test accuracy, test precision, test recall:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used ‘adam’ as our optimizer as it’s computationally efficient and is well suited for problems with a high number of parameters and ‘binary_crossentropy’ as our loss function as it’s most appropriate for our binary classification problem. For our evaluation, we’ll not only focus on accuracy as a metric but we’ll assess precision and recall too. Now let’s have a look at how the last 10 epochs went and how well our model performed on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
