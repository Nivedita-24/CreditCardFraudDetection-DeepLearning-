{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Credit Card Fraud using Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, we'll be leveraging the power of deep learning to solve a key issue that credit card companies often have to address, namely detecting fradulent transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Python Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.model_selection as model_selection\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CreditCard = pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total no. of records in the dataset are 284807\n",
      "Total features in the dataset are 31\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>'0'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>'0'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62    '0'  \n",
       "1  0.125895 -0.008983  0.014724    2.69    '0'  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66    '0'  \n",
       "3 -0.221929  0.062723  0.061458  123.50    '0'  \n",
       "4  0.502292  0.219422  0.215153   69.99    '0'  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total no. of records in the dataset are\", CreditCard.shape[0])\n",
    "print(\"Total features in the dataset are\", CreditCard.shape[1])\n",
    "CreditCard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## To check missing values in the dataset.\n",
    "CreditCard.isnull().values.any() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output shows that our dataset has no missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of Fraudulent Transactions in the dataset are 0.17 %\n"
     ]
    }
   ],
   "source": [
    "## Rename Class\n",
    "CreditCard.rename(columns ={'Class': \"isFraud\"}, inplace = True)\n",
    "CreditCard = CreditCard.applymap(lambda x: x.replace(\"'\", \"\") if (isinstance(x, str)) else x)\n",
    "CreditCard['isFraud'] = pd.to_numeric(CreditCard['isFraud'])\n",
    "\n",
    "# fraudulent Transactions Percentage\n",
    "fraud_per = CreditCard[CreditCard.isFraud == 1].isFraud.count() / CreditCard.isFraud.count()*100\n",
    "print(\"Percentage of Fraudulent Transactions in the dataset are {:.2f} %\".format(fraud_per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got the dataset from Kaggle and it contains two days worth of transactions by European cardholders. Due to cdonfidential nature of the data, a PCA transformation was done on 28 features and we have no information on what those features are. The only features that haven't undergone this transformation and we can identify them are 'Time', 'Amount', and 'Class'.\n",
    "\n",
    "Time represents the seconds elapsed between each transaction and the first transaction in the dataset. 'Amount denotes the amount of transaction anjd 'Class' refers to out target variable with 0 referring to a normal transaction and 1 referring to a fraudulent one.\n",
    "\n",
    "It is important to note here that the target variable's instances are imbalanced. Only 0.17% of transactions are fraudulent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Do fraudulent transactions occur more often during certain time?\n",
    "f, (ax1, ax2) = plt.subplots(2,1, sharex = True)\n",
    "f.suptitle('Time of transaction vs Amount by class')\n",
    "\n",
    "ax1.scatter(CreditCard[CreditCard.isFraud == 1].Time, CreditCard[CreditCard.isFraud == 1].Amount)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.scatter(CreditCard[CreditCard.isFraud == 0].Time, CreditCard[CreditCard.isFraud == 0].Amount)\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Time in Seconds')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't seem like the time of transaction really matters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataPreprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining x and y\n",
    "x = CreditCard.iloc[:,:-1].values\n",
    "y = CreditCard.iloc[:,:1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining training and tesing set\n",
    "## Train-Test split\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(x, y, test_size =0.1, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standardising the dataset as this would speedup the training process\n",
    "\n",
    "## Standardization\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.fit_transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned previously, this dataset is highly imbalanced. We'll address this issue using Synthetic Minority Oversampling Technique (SMOTE). This technique creates artificial minority class samples by replicating them. In this case it will create synthetic fraud instances and so corrects the imbalance in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected n_neighbors <= n_samples,  but n_samples = 2, n_neighbors = 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-567369409c52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moversample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_train_SMOTE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_SMOTE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moversample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# ## SMOTE plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     81\u001b[0m         )\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         y_ = (label_binarize(output[1], np.unique(y))\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/imblearn/over_sampling/_smote.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mnns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             X_new, y_new = self._make_samples(\n\u001b[1;32m    734\u001b[0m                 \u001b[0mX_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/neighbors/_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;34m\"Expected n_neighbors <= n_samples, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m                 \u001b[0;34m\" but n_samples = %d, n_neighbors = %d\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mn_samples_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_neighbors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m             )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected n_neighbors <= n_samples,  but n_samples = 2, n_neighbors = 6"
     ]
    }
   ],
   "source": [
    "## SMOTE\n",
    "# sm = SMOTE(random_state = 2)\n",
    "# X_train_SMOTE, y_train_SMOTE = sm.fit_resample(X_train,y_train)\n",
    "\n",
    "oversample = SMOTE()\n",
    "X_train_SMOTE, y_train_SMOTE = oversample.fit_resample(X_train, y_train)\n",
    "\n",
    "# ## SMOTE plot\n",
    "# pd.Series(y_train_SMOTE).value_counts().plot(kind = \"bar\")\n",
    "# plt.title(\"Balanced Dataset\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ANN Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weâ€™ve come to this number of neurons and layers in our network using a trial and error approach. We also used ReLU as our activation function for the hidden layers and a sigmoid function for our output layer. We've used multiple droput layers to prevent our network overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               3968      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 15,905\n",
      "Trainable params: 15,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "## DNN\n",
    "model = keras.Sequential([\n",
    "    tf.keras.layers.Dense(input_dim = 30, units =128, activation =\"relu\"),\n",
    "    tf.keras.layers.Dense(units = 64, activation = \"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units = 32, activation =\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units = 32, activation =\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units = 16, activation =\"relu\"),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(units = 1, activation =\"sigmoid\")])\n",
    "model.summary()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "15994/15994 [==============================] - 30s 2ms/step - loss: 0.0189 - Accuracy: 0.2399 - Precision: 0.9939 - Recall: 0.9955: 2s - loss: 0.0198 - Accuracy: 0.24\n",
      "Epoch 2/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0063 - Accuracy: 0.2453 - Precision: 0.9982 - Recall: 0.9992\n",
      "Epoch 3/50\n",
      "15994/15994 [==============================] - 40s 2ms/step - loss: 0.0051 - Accuracy: 0.2920 - Precision: 0.9987 - Recall: 0.9995\n",
      "Epoch 4/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0039 - Accuracy: 0.3401 - Precision: 0.9990 - Recall: 0.9996\n",
      "Epoch 5/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0039 - Accuracy: 0.4160 - Precision: 0.9989 - Recall: 0.9996: 1s - loss: 0.0038 - Accuracy: 0.411\n",
      "Epoch 6/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0035 - Accuracy: 0.4396 - Precision: 0.9991 - Recall: 0.9997\n",
      "Epoch 7/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0031 - Accuracy: 0.5067 - Precision: 0.9991 - Recall: 0.9997: 4s - loss: 0.0032 - Acc - ETA: 2s - loss: 0.0031 - Accuracy: 0.50\n",
      "Epoch 8/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0026 - Accuracy: 0.5673 - Precision: 0.9993 - Recall: 0.9997\n",
      "Epoch 9/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0026 - Accuracy: 0.5914 - Precision: 0.9993 - Recall: 0.9998\n",
      "Epoch 10/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0025 - Accuracy: 0.6501 - Precision: 0.9993 - Recall: 0.9998\n",
      "Epoch 11/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0025 - Accuracy: 0.6268 - Precision: 0.9993 - Recall: 0.9998: 2s - loss: 0.0024 - Accuracy: 0.6\n",
      "Epoch 12/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0025 - Accuracy: 0.6697 - Precision: 0.9994 - Recall: 0.9998\n",
      "Epoch 13/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0020 - Accuracy: 0.7008 - Precision: 0.9994 - Recall: 0.9999\n",
      "Epoch 14/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0022 - Accuracy: 0.7048 - Precision: 0.9994 - Recall: 0.9998\n",
      "Epoch 15/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0023 - Accuracy: 0.6683 - Precision: 0.9994 - Recall: 0.9998\n",
      "Epoch 16/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0020 - Accuracy: 0.7180 - Precision: 0.9995 - Recall: 0.9998: 3s - loss: 0.0021 -  - ETA: 0s - loss: 0.0020 - Accuracy: 0.7170 - Precision: 0.9995 - \n",
      "Epoch 17/50\n",
      "15994/15994 [==============================] - 31s 2ms/step - loss: 0.0021 - Accuracy: 0.7356 - Precision: 0.9995 - Recall: 0.9999\n",
      "Epoch 18/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0026 - Accuracy: 0.7193 - Precision: 0.9995 - Recall: 0.9998: 9s - loss: 0.0023  - ETA: 2s - loss: 0.0027 - A\n",
      "Epoch 19/50\n",
      "15994/15994 [==============================] - 31s 2ms/step - loss: 0.0019 - Accuracy: 0.7493 - Precision: 0.9995 - Recall: 0.9999: 2s - loss: 0.\n",
      "Epoch 20/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0017 - Accuracy: 0.7935 - Precision: 0.9995 - Recall: 0.9999: 1s - loss: 0.0017 - Accuracy: 0.7941 - Precisi\n",
      "Epoch 21/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0021 - Accuracy: 0.7461 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 22/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0018 - Accuracy: 0.7804 - Precision: 0.9995 - Recall: 0.9999: 2s - loss: 0.0018 - Accuracy: 0.7812 - Precision: 0.9995 - Recall:  - ETA: 2s - loss: 0.0018 - Accuracy: 0.7813 - Precision: 0.9995 -  - ETA: 1s - loss: 0.0018 - Accuracy: 0.7804 - Preci\n",
      "Epoch 23/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0019 - Accuracy: 0.7780 - Precision: 0.9996 - Recall: 0.9999: 0s - loss: 0.0019 - Accuracy: 0.7787 - Precision: 0.9996 - Recall: 0\n",
      "Epoch 24/50\n",
      "15994/15994 [==============================] - 31s 2ms/step - loss: 0.0018 - Accuracy: 0.7972 - Precision: 0.9995 - Recall: 0.9998\n",
      "Epoch 25/50\n",
      "15994/15994 [==============================] - 1209s 76ms/step - loss: 0.0017 - Accuracy: 0.7806 - Precision: 0.9996 - Recall: 0.9998\n",
      "Epoch 26/50\n",
      "15994/15994 [==============================] - 41s 3ms/step - loss: 0.0017 - Accuracy: 0.8101 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 27/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0018 - Accuracy: 0.8328 - Precision: 0.9996 - Recall: 0.9998\n",
      "Epoch 28/50\n",
      "15994/15994 [==============================] - 30s 2ms/step - loss: 0.0020 - Accuracy: 0.8402 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 29/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0019 - Accuracy: 0.8323 - Precision: 0.9995 - Recall: 0.9999\n",
      "Epoch 30/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0018 - Accuracy: 0.8069 - Precision: 0.9996 - Recall: 0.9998: 3s - loss: 0.0017 - Accuracy: 0.8020 - Precision: 0.9996 - ETA: 2s - loss: 0.0017 - Accura\n",
      "Epoch 31/50\n",
      "15994/15994 [==============================] - 32s 2ms/step - loss: 0.0014 - Accuracy: 0.8479 - Precision: 0.9996 - Recall: 0.9998: 5s - loss:  - ETA: 2s - loss: 0.0014 - A\n",
      "Epoch 32/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0013 - Accuracy: 0.8601 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 33/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0021 - Accuracy: 0.8256 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 34/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0017 - Accuracy: 0.8307 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 35/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0019 - Accuracy: 0.8177 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 36/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0017 - Accuracy: 0.8745 - Precision: 0.9996 - Recall: 0.9998: 2s - loss: 0.0017 - Accura\n",
      "Epoch 37/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0017 - Accuracy: 0.8608 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 38/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0021 - Accuracy: 0.8557 - Precision: 0.9996 - Recall: 0.9998: 10s - loss: 0.0022 - Accuracy: 0.8632 - Precision: 0.999 - ETA: 9s - loss: 0.0022 - Accuracy: 0.8619 - Precis - ETA: 0s - loss: 0.0021 - Accuracy: 0.8568 - Precision: 0.9996 - Recall: \n",
      "Epoch 39/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0021 - Accuracy: 0.8203 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 40/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0017 - Accuracy: 0.8396 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 41/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0020 - Accuracy: 0.8330 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 42/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0013 - Accuracy: 0.8842 - Precision: 0.9997 - Recall: 0.9999\n",
      "Epoch 43/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0022 - Accuracy: 0.8437 - Precision: 0.9995 - Recall: 0.9998\n",
      "Epoch 44/50\n",
      "15994/15994 [==============================] - 35s 2ms/step - loss: 0.0016 - Accuracy: 0.8829 - Precision: 0.9997 - Recall: 0.9999: 1s - loss: 0.0016 - Accuracy: 0.8833 - Precisi\n",
      "Epoch 45/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0015 - Accuracy: 0.9082 - Precision: 0.9997 - Recall: 0.9999: 4s - loss: 0.0014 - Accuracy: 0.9077 - Precision: 0.9997 - Recall: 0. -\n",
      "Epoch 46/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0015 - Accuracy: 0.9013 - Precision: 0.9997 - Recall: 0.9999\n",
      "Epoch 47/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0018 - Accuracy: 0.8983 - Precision: 0.9997 - Recall: 0.9999\n",
      "Epoch 48/50\n",
      "15994/15994 [==============================] - 34s 2ms/step - loss: 0.0017 - Accuracy: 0.8513 - Precision: 0.9996 - Recall: 0.9999\n",
      "Epoch 49/50\n",
      "15994/15994 [==============================] - 33s 2ms/step - loss: 0.0017 - Accuracy: 0.9080 - Precision: 0.9997 - Recall: 0.9999\n",
      "Epoch 50/50\n",
      "15994/15994 [==============================] - 29s 2ms/step - loss: 0.0019 - Accuracy: 0.9029 - Precision: 0.9997 - Recall: 0.9999: 1s - loss: 0.0019 - Accuracy: 0.900\n",
      "Evaluate on test data\n",
      "891/891 [==============================] - 1s 1ms/step - loss: 0.0563 - Accuracy: 0.9699 - Precision: 0.7759 - Recall: 0.8182\n",
      "test loss, test accuracy, test precision, test recall: [0.05626910924911499, 0.9699097871780396, 0.7758620977401733, 0.8181818127632141]\n"
     ]
    }
   ],
   "source": [
    "## Metrics\n",
    "metrics = [\n",
    "    tf.keras.metrics.Accuracy(name = \"Accuracy\"),\n",
    "    tf.keras.metrics.Precision(name = \"Precision\"),\n",
    "    tf.keras.metrics.Recall(name =\"Recall\")]\n",
    "\n",
    "## Compiling and Fiting the model\n",
    "model.compile(optimizer =\"adam\",loss = \"binary_crossentropy\",\n",
    "             metrics = metrics)\n",
    "model.fit(X_train_SMOTE, y_train_SMOTE, batch_size = 32, epochs = 50)\n",
    "\n",
    "print(\"Evaluate on test data\")\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print(\"test loss, test accuracy, test precision, test recall:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used â€˜adamâ€™ as our optimizer as itâ€™s computationally efficient and is well suited for problems with a high number of parameters and â€˜binary_crossentropyâ€™ as our loss function as itâ€™s most appropriate for our binary classification problem. For our evaluation, weâ€™ll not only focus on accuracy as a metric but weâ€™ll assess precision and recall too. Now letâ€™s have a look at how the last 10 epochs went and how well our model performed on our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
